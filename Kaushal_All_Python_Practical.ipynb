{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tYZ_lbFYFN8",
        "outputId": "e928c996-2a7c-47ca-a4af-677dcfd37073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=95527b77e0d17f047ac7c727625f75d3ed82e14e4474a39f556152c3bbf67b09\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 1: PySpark Data Reading and Display Example\n",
        "\n",
        "# Import necessary PySpark modules\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataReadingExample\").getOrCreate()\n",
        "\n",
        "# Read data from a CSV file into a DataFrame\n",
        "data_file = \"songs.csv\"\n",
        "df = spark.read.csv(data_file, header=True, inferSchema=True)\n",
        "\n",
        "# Show the first few rows of the DataFrame\n",
        "df.show(3)\n",
        "\n",
        "# Stop the SparkSession when you're done\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__5FoHq6YhKY",
        "outputId": "0ff4a477-747b-43f2-959f-49043a3b3b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+--------+-------+\n",
            "|song| title|  artist|  album|\n",
            "+----+------+--------+-------+\n",
            "|   1|Song 1|Artist 1|Album 1|\n",
            "|   2|Song 2|Artist 2|Album 2|\n",
            "|   3|Song 3|Artist 3|Album 3|\n",
            "+----+------+--------+-------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 2: Combining DataFrames with PySpark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"CombiningDataFramesExample\").getOrCreate()\n",
        "\n",
        "# Create two example DataFrames\n",
        "data1 = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
        "data2 = [(4, \"David\"), (5, \"Eve\"), (6, \"Frank\")]\n",
        "columns = [\"id\", \"name\"]\n",
        "df1 = spark.createDataFrame(data1, columns)\n",
        "df2 = spark.createDataFrame(data2, columns)\n",
        "\n",
        "# Union two DataFrames (stack them on top of each other) and Show the combined DataFrame\n",
        "print(\"Union\")\n",
        "combined_df = df1.union(df2)\n",
        "combined_df.show()\n",
        "\n",
        "# Join two DataFrames\n",
        "data3 = [(1, \"Math\"), (2, \"English\"), (3, \"Science\")]\n",
        "subjects_columns = [\"id\", \"subject\"]\n",
        "df3 = spark.createDataFrame(data3, subjects_columns)\n",
        "\n",
        "# Inner join the DataFrames on the 'id' column\n",
        "print(\"Inner Join\")\n",
        "joined_df = df1.join(df3, \"id\", \"inner\")\n",
        "joined_df.show()\n",
        "\n",
        "# Stop the SparkSession when you're done\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrR96ypBZekW",
        "outputId": "6fd1a0d9-36f4-475e-8bbe-240731912b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Union\n",
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|  Alice|\n",
            "|  2|    Bob|\n",
            "|  3|Charlie|\n",
            "|  4|  David|\n",
            "|  5|    Eve|\n",
            "|  6|  Frank|\n",
            "+---+-------+\n",
            "\n",
            "Inner Join\n",
            "+---+-------+-------+\n",
            "| id|   name|subject|\n",
            "+---+-------+-------+\n",
            "|  1|  Alice|   Math|\n",
            "|  2|    Bob|English|\n",
            "|  3|Charlie|Science|\n",
            "+---+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pratical 3: Combining DataFrames with PySpark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"MapReduceExample\").getOrCreate()\n",
        "\n",
        "# Create an RDD (Resilient Distributed Dataset) from a list of numbers\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Collect the RDD and convert it to a list\n",
        "collected_data = rdd.collect()\n",
        "print(\"Collected Data:\", collected_data)\n",
        "\n",
        "# Filter out even numbers from the RDD\n",
        "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "filtered_data = filtered_rdd.collect()\n",
        "print(\"Filtered Data (Even numbers):\", filtered_data)\n",
        "\n",
        "# Map operation: Square each element of the RDD\n",
        "mapped_rdd = rdd.map(lambda x: x * x)\n",
        "mapped_data = mapped_rdd.collect()\n",
        "print(\"Mapped Data (Squared):\", mapped_data)\n",
        "\n",
        "# Map-Reduce operation: Sum all the elements in the RDD\n",
        "reduced_result = rdd.reduce(lambda x, y: x + y)\n",
        "print(\"Reduced Result (Sum):\", reduced_result)\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1a2vmMAacam",
        "outputId": "8296f259-f8c0-42be-eecf-d6b7e789a2d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected Data: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Filtered Data (Even numbers): [2, 4, 6, 8]\n",
            "Mapped Data (Squared): [1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
            "Reduced Result (Sum): 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 4: Creating a spark session using the configuration and Dataframe creation\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Configure Spark\n",
        "spark = SparkSession.builder.appName(\"CustomConfigExample\").config(\"spark.executor.memory\", \"2g\").getOrCreate()\n",
        "\n",
        "\n",
        "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
        "columns = [\"id\", \"name\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Stop the Spark session when done\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4zrnJLPieWZ",
        "outputId": "a635d5c3-8aa8-4dad-fd28-10f800592a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|  Alice|\n",
            "|  2|    Bob|\n",
            "|  3|Charlie|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 5: PySpark Word Count and Data Manipulation Example\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"WordCountExample\").getOrCreate()\n",
        "\n",
        "# Create an RDD from a text file (replace 'your_text_file.txt' with your file)\n",
        "text_file = \"textfile.txt\"\n",
        "rdd = spark.sparkContext.textFile(text_file)\n",
        "\n",
        "# Split the lines into words, perform word count, and collect the results\n",
        "word_counts = rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).collect()\n",
        "\n",
        "# Display the word count results\n",
        "for word, count in word_counts:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUICIwWjhuYI",
        "outputId": "2437b191-264c-45f8-fa4f-2716286c63a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: 1\n",
            "is: 1\n",
            "prachi: 2\n",
            "i: 1\n",
            "am: 1\n",
            "my: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 7: Creating a temporary view of DataFrame to use SQL Query with Spark Session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQLExample\").getOrCreate()\n",
        "\n",
        "# Read data into a DataFrame (Replace 'your_data_file.csv' with your file)\n",
        "data_file = \"songs.csv\"\n",
        "df = spark.read.csv(data_file, header=True, inferSchema=True)\n",
        "\n",
        "# Register the DataFrame as a temporary view\n",
        "df.createOrReplaceTempView(\"my_temp_view\")\n",
        "\n",
        "# Run SQL queries against the temporary view\n",
        "result = spark.sql(\"SELECT * FROM my_temp_view WHERE song = 4\")\n",
        "result.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4lq3bV0sy9T",
        "outputId": "f4b27cb4-ee57-481c-b15c-6974d7431457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+--------+-------+\n",
            "|song| title|  artist|  album|\n",
            "+----+------+--------+-------+\n",
            "|   4|Song 4|Artist 4|Album 4|\n",
            "+----+------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 8: Creating user defined function with Spark Session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"UDFExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\",), (\"Bob\",), (\"Charlie\",)]\n",
        "columns = [\"name\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Define a UDF\n",
        "def greet(name):\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "# Register the UDF\n",
        "greet_udf = udf(greet, StringType())\n",
        "\n",
        "# Apply the UDF to the DataFrame\n",
        "df_with_greeting = df.withColumn(\"greeting\", greet_udf(df[\"name\"]))\n",
        "\n",
        "# Show the result\n",
        "df_with_greeting.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OnkxLJjoy1D",
        "outputId": "d3c538f6-3cea-49ce-e4c0-839ce3f531d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------+\n",
            "|   name|       greeting|\n",
            "+-------+---------------+\n",
            "|  Alice|  Hello, Alice!|\n",
            "|    Bob|    Hello, Bob!|\n",
            "|Charlie|Hello, Charlie!|\n",
            "+-------+---------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}